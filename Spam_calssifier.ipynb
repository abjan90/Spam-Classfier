{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d43534dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request \n",
    "import tarfile\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\" , \"spam\") #It will create a path datasets/spam\n",
    "\n",
    "#Fetched the emails\n",
    "def fetch_spam_data(spam_url = SPAM_URL , spam_path = SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for file_name , urls in ((\"ham.tar.bz2\" , HAM_URL) , (\"spam.tar.bz2\" , SPAM_URL)):\n",
    "        path = os.path.join(spam_path , file_name) #It will create a path datasets/spam/file_name \n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve( urls , path) #This will download the file and then store it in the path\n",
    "        tar_bz2_file = tarfile.open(path) #The downloaded fie will be in the format of the .tar so we need tarfile\n",
    "        tar_bz2_file.extractall(spam_path) #Then it will extract the file\n",
    "        tar_bz2_file.close() \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d56ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c363dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaded the email\n",
    "HAM_DIR = os.path.join(SPAM_PATH , \"easy_ham\") #datasets\\spam\\easy_ham\n",
    "SPAM_DIR = os.path.join(SPAM_PATH , \"spam\") #'datasets\\\\spam\\\\spam'\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26299285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy \n",
    "\n",
    "def load_email(is_spam , filename , spam_path = SPAM_PATH): \n",
    "    directory = \"spam\" if is_spam else \"easy_ham\" #esley chahi kun email read bhanera select garcha\n",
    "\n",
    "    with open  (os.path.join(spam_path , directory , filename) , \"rb\") as f:   # open .. as f allows to read the email and close when its done as rb-> read in bianry\n",
    "        return email.parser.BytesParser(policy = email.policy.default).parse(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "675431c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False , filename=name) for name in ham_filenames ]\n",
    "spam_emails = [load_email(is_spam = True , filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e8be867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<email.message.EmailMessage object at 0x000001A07767E570>, <email.message.EmailMessage object at 0x000001A07767C110>, <email.message.EmailMessage object at 0x000001A07767CD70>, <email.message.EmailMessage object at 0x000001A07767EC00>, <email.message.EmailMessage object at 0x000001A07767F770>]\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f44851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email , str): \n",
    "        return email\n",
    "    payload = email.get_payload() #Here if the email is multipart then it will contain contain content of each subpart\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\",\".join([get_email_structure(sub_email) for sub_email in payload]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d56187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter() #Iniatializes an empty dictionary\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1  \n",
    "    return structures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffb63959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain,text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain,image/jpeg)', 3),\n",
       " ('multipart(text/html,application/octet-stream)', 2),\n",
       " ('multipart(text/plain,application/octet-stream)', 1),\n",
       " ('multipart(text/html,text/plain)', 1),\n",
       " ('multipart(multipart(text/html),application/octet-stream,image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain,text/html),image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()\n",
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dba28f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ham_emails + spam_emails\n",
    "Y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b255c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(X , Y , test_size= 0.2 , random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7bceb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #esley chahi string accept garcha hai \n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):  #This accepts only string\n",
    "\n",
    "    soup = BeautifulSoup(html , 'lxml')\n",
    "\n",
    "    if soup.head: #Esley Head remove garcha\n",
    "        soup.head.decompose()\n",
    "\n",
    "\n",
    "    for a in soup.find_all(\"a\"): #This replaces all the <a> tags with Hyperlink\n",
    "        a.replace_with(\" HYPERLINK \")\n",
    "\n",
    "\n",
    "    for tags in soup.find_all():\n",
    "        if tags.name == \"a\":\n",
    "            continue\n",
    "        if tags.name == \"head\":\n",
    "            continue\n",
    "        if tags.string:\n",
    "            continue\n",
    "        tags.unwrap()\n",
    "\n",
    "    text = soup.get_text()\n",
    "\n",
    "    return unescape(text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99e64e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text_complete(msg):\n",
    "    \"\"\"Gets ALL text content from email, not just first part\"\"\"\n",
    "    all_text = []\n",
    "    \n",
    "    for part in msg.walk():\n",
    "        content_type = part.get_content_type()\n",
    "        \n",
    "        if content_type in [\"text/html\", \"text/plain\"]:\n",
    "            text = part.get_payload(decode=True)\n",
    "            if text:\n",
    "                all_text.append(text.decode(errors=\"ignore\"))\n",
    "    \n",
    "    return \" \".join(all_text)  # Combine ALL parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9374158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_spam_email = [email for email , label in zip(x_train , y_train) \n",
    "                     if label == 1 and get_email_structure(email) == 'text/html']\n",
    "sample_spam_email_1 = sample_spam_email[7] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a715a11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTC\n",
      "\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "\n",
      "For Immediate Release\n",
      "\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
      "Put CBYI on your watch list, acquire a position TODAY.\n",
      "\n",
      "REASONS TO INVEST IN CBYI\n",
      "\n",
      "A profitable company and is on track to beat ALL earnings estimates!\n",
      "\n",
      "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
      "\n",
      "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
      "\n",
      "RAPIDLY GROWING INDUSTRY\n",
      "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billion from \"smell technology\" by the end of 2003.\n",
      "\n",
      "!!!!!CONGRATULATIONS!!!!!Our last recommendation to buy ORBT at $1.29 rallied and is holding steady at $3.50! Congratulations to all our subscribers that took advantage of this recommendation.\n",
      "\n",
      "ALL removes HONORED. Please allow 7 days to be removed and send ALL addresses to:\n",
      "\n",
      " HYPERLINK  \n",
      "\n",
      "Certain statements contained in this news release may be forward-looking statements within the meaning of The Private Securities Litigation Reform Act of 1995. These statements may be identified by such terms as \"expect\", \"believe\", \"may\", \"will\", and \"intend\" or similar terms. We are NOT a registered investment advisor or a broker dealer. This is NOT an offer to buy or sell securities. No recommendation that the securities of the companies profiled should be purchased, sold or held by individuals or entities that learn of the profiled companies. We were paid $27,000 in cash by a third party to publish this report. Investing in companies profiled is high-risk and use of this information is for reading purposes only. If anyone decides to act as an investor, then it will be that investor's sole risk. Investors are advised NOT to invest without the proper advisement from an attorney or a registered financial broker. Do not rely solely on the information presented, do additional independent research to form your own opinion and decision regarding investing in the profiled companies. Be advised that the purchase of such high-risk securities may result in the loss of your entire investment.  Not intended for recipients or residents of CA,CO,CT,DE,ID, IL,IA,LA,MO,NV,NC,OK,OH,PA,RI,TN,VA,WA,WV,WI. Void where prohibited.  The owners of this publication may already own free trading shares in CBYI and may immediately sell all or a portion of these shares into the open market at or about the time this report is published.  Factual statements are made as of the date stated and are subject to change without notice.\n",
      "Copyright c 2001\n",
      "≡\n",
      "\n",
      "OTC\n",
      "≡\n",
      "\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "raw = email_to_text_complete(sample_spam_email_1)\n",
    "print(html_to_plain_text(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4a1184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "import nltk\n",
    "url = urlextract.URLExtract()\n",
    "stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "756c7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class email_to_word_count(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, \n",
    "                 url_replace=True, stemming=True, replace_numbers=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.url_replace = url_replace\n",
    "        self.stemming = stemming \n",
    "        self.replace_numbers = replace_numbers\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        X_transformed = []\n",
    "        for email in x: \n",
    "            text = email_to_text_complete(email) or \" \"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.url_replace and url is not None:\n",
    "                url_1 = list(set(url.find_urls(text)))\n",
    "                url_1.sort(key=lambda url: len(url), reverse=True)\n",
    "                for found_url in url_1:\n",
    "                    text = text.replace(found_url, \" url \")\n",
    "            if self.replace_numbers:\n",
    "                words = text.split()\n",
    "                new_word = []\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        float(word)\n",
    "                        new_word.append(\"NUMBER\")\n",
    "                    except ValueError:\n",
    "                        new_word.append(word)\n",
    "                text = \" \".join(new_word)  \n",
    "            \n",
    "            word_counts = Counter(text.split())  \n",
    "            \n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()  \n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a566aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'the': 15, 'pick': 9, 'number': 6, '-lbrace': 6, 'of': 5, '...': 5, '-rbrace': 5, 'i': 4, 'is': 4, '-list': 4, 'thi': 3, '+inbox': 3, '-subject': 3, 'ftp': 3, '-sequenc': 3, '18:19:04': 3, 'command': 3, 'delta$': 3, 'from': 3, '18:19:03': 2, '4852-4852': 2, 'mercuri': 2, 'hit': 2, \"that'\": 2, 'come': 2, 'version': 2, 'use': 2, 'on': 2, 'url': 2, 'and': 2, 'one': 2, 'date:': 1, 'wed,': 1, 'aug': 1, '10:54:46': 1, 'from:': 1, 'chri': 1, 'garrigu': 1, '<cwg-dated-1030377287.06fa6d@deepeddy.com>': 1, 'message-id:': 1, '<1029945287.4797.tmda@deepeddy.vircio.com>': 1, '|': 1, \"can't\": 1, 'reproduc': 1, 'error.': 1, 'for': 1, 'me': 1, 'it': 1, 'veri': 1, 'repeatable...': 1, '(like': 1, 'everi': 1, 'time,': 1, 'without': 1, 'fail).': 1, 'debug': 1, 'log': 1, 'happen': 1, 'pick_it': 1, '{exec': 1, '-rbrace}': 1, '{4852-4852': 1, 'mercury}': 1, 'exec': 1, 'ftoc_pickmsg': 1, '{{1': 1, 'hit}}': 1, 'mark': 1, 'tkerror:': 1, 'syntax': 1, 'error': 1, 'in': 1, 'express': 1, '\"int': 1, 'note,': 1, 'if': 1, 'run': 1, 'by': 1, 'hand': 1, 'where': 1, '\"1': 1, 'hit\"': 1, '(obviously).': 1, 'nmh': 1, \"i'm\": 1, '-version': 1, '--': 1, 'nmh-1.0.4': 1, '[compil': 1, 'at': 1, 'sun': 1, 'mar': 1, '14:55:56': 1, 'ict': 1, '2002]': 1, 'relev': 1, 'part': 1, 'my': 1, '.mh_profil': 1, 'mhparam': 1, '-seq': 1, 'sel': 1, 'sinc': 1, 'works,': 1, 'sequenc': 1, '(actually,': 1, 'both': 1, 'them,': 1, 'explicit': 1, 'line,': 1, 'search': 1, 'popup,': 1, 'that': 1, '.mh_profile)': 1, 'do': 1, 'get': 1, 'created.': 1, 'kre': 1, 'ps:': 1, 'still': 1, 'code': 1, 'form': 1, 'a': 1, 'day': 1, 'ago,': 1, \"haven't\": 1, 'been': 1, 'abl': 1, 'to': 1, 'reach': 1, 'cv': 1, 'repositori': 1, 'today': 1, '(local': 1, 'rout': 1, 'issu': 1, 'think).': 1, '_______________________________________________': 1, 'exmh-work': 1, 'mail': 1, 'list': 1, 'exmh-workers@redhat.com': 1})\n",
      " Counter({'the': 5, 'number': 4, 'a': 3, 'of': 3, 'and': 3, 'limeston': 2, 'mount': 2, 'from': 2, 'for': 2, 'as': 2, 'ft': 2, 'is': 2, 'thi': 2, 'yahoo!': 2, 'group': 2, 'url': 2, 'to': 2, 'martin': 1, 'posted:': 1, 'tasso': 1, 'papadopoulos,': 1, 'greek': 1, 'sculptor': 1, 'behind': 1, 'plan,': 1, 'judg': 1, 'that': 1, 'kerdylio,': 1, 'mile': 1, 'east': 1, 'salonika': 1, 'not': 1, 'far': 1, 'atho': 1, 'monast': 1, 'community,': 1, 'wa': 1, 'ideal': 1, 'patriot': 1, 'sculpture.': 1, 'well': 1, \"alexander'\": 1, 'granit': 1, 'features,': 1, 'high': 1, 'wide,': 1, 'museum,': 1, 'restor': 1, 'amphitheatr': 1, 'car': 1, 'park': 1, 'admir': 1, 'crowd': 1, 'are': 1, 'plan': 1, '---------------------': 1, 'so': 1, 'mountain': 1, 'or': 1, 'granite?': 1, 'if': 1, \"it'\": 1, 'limestone,': 1, \"it'll\": 1, 'weather': 1, 'pretti': 1, 'fast.': 1, '------------------------': 1, 'sponsor': 1, '---------------------~-->': 1, 'dvd': 1, 'free': 1, '+s&p': 1, 'join': 1, 'now': 1, '---------------------------------------------------------------------~->': 1, 'unsubscrib': 1, 'group,': 1, 'send': 1, 'an': 1, 'email': 1, 'to:': 1, 'forteana-unsubscribe@egroups.com': 1, 'your': 1, 'use': 1, 'subject': 1})\n",
      " Counter({'the': 16, 'and': 10, 'to': 10, 'man': 6, 'secur': 6, 'of': 5, 'an': 4, 'with': 4, 'servic': 4, 'said.': 4, 'a': 4, 'in': 3, 'moscow': 3, 'wa': 3, 'polic': 3, 'building,': 3, 'threaten': 2, 'explos': 2, 'thursday': 2, 'number': 2, 'offic': 2, 'seiz': 2, 'who': 2, 'he': 2, 'truck': 2, 'ntv': 2, 'talk': 2, 'interfax': 2, 'itar-tass': 2, 'news': 2, 'agenc': 2, 'one': 2, 'half': 2, 'negoti': 2, 'drove': 2, 'from': 2, 'yahoo!': 2, 'group': 2, 'url': 2, 'august': 1, '22,': 1, '1:40': 1, 'pm': 1, '(ap)': 1, '-': 1, 'on': 1, 'unidentifi': 1, 'said': 1, 'arm': 1, 'blow': 1, 'up': 1, 'hi': 1, 'front': 1, \"russia'\": 1, 'feder': 1, 'headquart': 1, 'moscow,': 1, 'televis': 1, 'reported.': 1, 'automat': 1, 'rifl': 1, 'carrying,': 1, 'then': 1, 'got': 1, 'out': 1, 'taken': 1, 'into': 1, 'custody,': 1, 'no': 1, 'other': 1, 'detail': 1, 'were': 1, 'immedi': 1, 'available.': 1, 'had': 1, 'demand': 1, 'high': 1, 'govern': 1, 'officials,': 1, 'ekho': 1, 'moskvi': 1, 'radio': 1, 'report': 1, 'that': 1, 'want': 1, 'russian': 1, 'presid': 1, 'vladimir': 1, 'putin.': 1, 'forc': 1, 'rush': 1, 'within': 1, 'block': 1, 'kremlin,': 1, 'red': 1, 'squar': 1, 'bolshoi': 1, 'ballet,': 1, 'surround': 1, 'man,': 1, 'claim': 1, 'have': 1, 'ton': 1, 'explosives,': 1, 'continu': 1, 'for': 1, 'about': 1, 'hour': 1, 'outsid': 1, 'reported,': 1, 'cite': 1, 'witnesses.': 1, 'later': 1, 'away': 1, 'under': 1, 'escort,': 1, 'street': 1, 'near': 1, \"moscow'\": 1, 'olymp': 1, 'penta': 1, 'hotel,': 1, 'where': 1, 'author': 1, 'held': 1, 'further': 1, 'him,': 1, 'press': 1, 'move': 1, 'appear': 1, 'be': 1, 'attempt': 1, 'by': 1, 'get': 1, 'him': 1, 'more': 1, 'location.': 1, '------------------------': 1, 'sponsor': 1, '---------------------~-->': 1, 'dvd': 1, 'free': 1, '+s&p': 1, 'join': 1, 'now': 1, '---------------------------------------------------------------------~->': 1, 'unsubscrib': 1, 'thi': 1, 'group,': 1, 'send': 1, 'email': 1, 'to:': 1, 'forteana-unsubscribe@egroups.com': 1, 'your': 1, 'use': 1, 'is': 1, 'subject': 1})\n",
      " Counter({'the': 9, 'of': 7, 'that': 6, 'klez': 5, 'number': 5, 'and': 4, 'viru': 3, 'most': 3, 'to': 3, 'it': 3, 'virus': 3, 'a': 3, 'alreadi': 2, 'prolif': 2, 'pc': 2, 'approach': 2, 'ever.': 2, 'new': 2, 'in': 2, 'url': 2, 'klez:': 1, \"won't\": 1, 'die': 1, 'ever,': 1, 'continu': 1, 'wreak': 1, 'havoc.': 1, 'andrew': 1, 'brandt': 1, '>>from': 1, 'septemb': 1, 'issu': 1, 'world': 1, 'magazin': 1, 'post': 1, 'thursday,': 1, 'august': 1, '01,': 1, 'worm': 1, 'is': 1, 'seventh': 1, 'month': 1, 'wriggl': 1, 'across': 1, 'web,': 1, 'make': 1, 'one': 1, 'persist': 1, 'expert': 1, 'warn': 1, 'may': 1, 'be': 1, 'harbing': 1, 'use': 1, 'combin': 1, 'pernici': 1, 'go': 1, 'from': 1, 'pc.': 1, 'antiviru': 1, 'softwar': 1, 'maker': 1, 'symantec': 1, 'mcafe': 1, 'both': 1, 'report': 1, 'more': 1, 'than': 1, 'infect': 1, 'daily,': 1, 'with': 1, 'no': 1, 'sign': 1, 'letup': 1, 'at': 1, 'press': 1, 'time.': 1, 'british': 1, 'secur': 1, 'firm': 1, 'messagelab': 1, 'estim': 1, 'everi': 1, 'e-mail': 1, 'messag': 1, 'hold': 1, 'variat': 1, 'virus,': 1, 'say': 1, 'ha': 1, 'surpass': 1, 'last': 1, \"summer'\": 1, 'sircam': 1, 'as': 1, 'some': 1, 'newer': 1, 'variant': 1, \"aren't\": 1, 'mere': 1, 'nuisances--they': 1, 'can': 1, 'carri': 1, 'other': 1, 'them': 1, 'corrupt': 1, 'your': 1, 'data.': 1, '...': 1, '_______________________________________________': 1, 'irregular': 1, 'mail': 1, 'list': 1, 'irregulars@tb.tf': 1})\n",
      " Counter({'to': 7, 'of': 5, 'the': 4, 'use': 4, 'a': 3, 'url': 3, '>': 2, 'in': 2, 'as': 2, 'make': 2, 'i': 2, 'carbonara': 2, 'is': 2, 'and': 2, \"i'v\": 2, 'seen': 2, 'recip': 2, 'creme': 2, 'it': 2, 'an': 2, 'for': 2, 'yahoo!': 2, 'group': 2, 'ad': 1, 'cream': 1, 'spaghetti': 1, 'carbonara,': 1, 'which': 1, 'ha': 1, 'same': 1, 'effect': 1, 'on': 1, 'pasta': 1, 'pizza': 1, 'deep-pie;': 1, 'just': 1, 'had': 1, 'jump': 1, 'here': 1, 'one': 1, 'my': 1, 'favourit': 1, 'ask': 1, 'what': 1, 'hell': 1, 'are': 1, 'you': 1, 'suppos': 1, 'instead': 1, 'cream?': 1, 'never': 1, 'that': 1, \"hasn't\": 1, 'this.': 1, 'person': 1, 'low': 1, 'fat': 1, 'fraich': 1, 'becaus': 1, 'work': 1, 'quit': 1, 'nice': 1, 'but': 1, 'onli': 1, 'time': 1, 'supposedli': 1, 'authent': 1, 'wa': 1, 'ident': 1, 'mine': 1, '(cream,': 1, 'egg': 1, 'lot': 1, 'fresh': 1, 'parmesan)': 1, 'except': 1, 'fraiche.': 1, 'stew': 1, '--': 1, 'stewart': 1, 'smith': 1, 'scottish': 1, 'microelectron': 1, 'centre,': 1, 'univers': 1, 'edinburgh.': 1, '------------------------': 1, 'sponsor': 1, '---------------------~-->': 1, 'number': 1, 'dvd': 1, 'free': 1, '+s&p': 1, 'join': 1, 'now': 1, '---------------------------------------------------------------------~->': 1, 'unsubscrib': 1, 'from': 1, 'thi': 1, 'group,': 1, 'send': 1, 'email': 1, 'to:': 1, 'forteana-unsubscribe@egroups.com': 1, 'your': 1, 'subject': 1})]\n"
     ]
    }
   ],
   "source": [
    "x_demo = X[:5]\n",
    "x_hero = email_to_word_count().fit_transform(x_demo)\n",
    "print(x_hero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bb4ea531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class word_count_to_vector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        total_count = Counter()\n",
    "        \n",
    "        \n",
    "        for word_count in x:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        \n",
    "        \n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        \n",
    "        for row, word_count in enumerate(x):\n",
    "            for word, count in word_count.items():\n",
    "                if word in self.vocabulary_:\n",
    "                    rows.append(row)\n",
    "                    cols.append(self.vocabulary_[word])\n",
    "                    data.append(count)\n",
    "        \n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(x), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79e9c2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int32'\n",
       "\twith 46 stored elements and shape (5, 11)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hola = word_count_to_vector(vocabulary_size=10)\n",
    "x_hero_vector = hola.fit_transform(x_hero)\n",
    "x_hero_vector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd4cae33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 15,  5,  1,  2,  6,  1,  2,  4,  1,  9],\n",
       "       [ 0,  5,  3,  2,  3,  4,  3,  2,  2,  1,  0],\n",
       "       [ 0, 16,  5, 10, 10,  2,  4,  2,  1,  1,  0],\n",
       "       [ 0,  9,  7,  3,  4,  5,  3,  2,  1,  6,  0],\n",
       "       [ 0,  4,  5,  7,  2,  1,  3,  3,  2,  1,  0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hero_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f88b5c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'to': 3,\n",
       " 'and': 4,\n",
       " 'number': 5,\n",
       " 'a': 6,\n",
       " 'url': 7,\n",
       " 'is': 8,\n",
       " 'that': 9,\n",
       " 'pick': 10}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hola.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3aa7c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "    (\"email_to_word_count\" , email_to_word_count()),\n",
    "    (\"word_count_to_vector\" , word_count_to_vector(vocabulary_size= 1000))\n",
    "])\n",
    "\n",
    "x_train_transformed = Pipeline.fit_transform(x_train)\n",
    "x_test_transformed = Pipeline.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "11ea6510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 95.74%\n",
      "Recall: 94.74%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score , recall_score\n",
    "\n",
    "log_train = LogisticRegression(solver = 'liblinear' , random_state=42 , max_iter=1000)\n",
    "log_train.fit(x_train_transformed , y_train)\n",
    "\n",
    "y_prediction = log_train.predict(x_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_prediction)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_prediction)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
